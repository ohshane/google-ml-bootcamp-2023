{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word representation\n",
    "The easiest way to embed a word is one-hot.\n",
    "\n",
    "$$\n",
    "\\mathbf x =\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "1 \\\\\n",
    "\\vdots \\\\\n",
    "0 \n",
    "\\end{bmatrix}_{||V|| \\times 1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analogy of two vectors\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "e_{\\text{man}} - e_{\\text{woman}} &\\approx e_{\\text{king}} - e \\\\\n",
    "e &\\approx e_{\\text{king}} - e_{\\text{man}} + e_{\\text{woman}} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The similarity of the vectors are calculated with cosine similarity.\n",
    "\n",
    "$$\n",
    "-1 \\leq\\mathrm{cossim}(e, e_{\\text{king}} - e_{\\text{man}} + e_{\\text{woman}}) \\leq 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# embedding matrix\n",
    "E = np.random.randn(10, 100)\n",
    "\n",
    "# one-hot vector \n",
    "x = np.zeros((100,1))\n",
    "x[42] += 1\n",
    "\n",
    "# embedded vector\n",
    "e = E @ x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(text) = 1093\n",
      "len(X)    = 4356\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"John quickly realized that the fox was jumping over a brown fence. Meanwhile, the lazy dog slept under the warm sun, dreaming of chasing squirrels in the park. A wizard in a distant land cast spells to levitate objects and summon mystical creatures. The gym was full of athletes lifting weights, running on treadmills, and practicing yoga poses. Buzzing bees were collecting nectar from vibrant flowers, while a group of birds sang harmoniously from the treetops. In the city, cars zoomed by as people hurried to work, their minds filled with tasks and deadlines. The library was a sanctuary of knowledge, where students pored over books and researchers delved into ancient manuscripts. A chef in a bustling kitchen prepared exquisite dishes, skillfully chopping vegetables and grilling meats. At the beach, waves crashed against the shore as children built sandcastles and surfers rode the swells. In the forest, a lumberjack wielded his axe, cutting down trees for timber. The night sky was a tapestry of stars, constellations, and planets, inspiring wonder and awe in all who gazed upon it.\"\"\"\n",
    "text = text.lower()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "c2i = lambda c: chars.index(c)\n",
    "i2c = lambda i: chars[i]\n",
    "\n",
    "X = []\n",
    "context_size = 2\n",
    "for i in range(context_size, len(text)-context_size):\n",
    "    for offset in range(-context_size, context_size+1):\n",
    "        if offset == 0:\n",
    "            continue\n",
    "        X.append([c2i(text[i]), c2i(text[i+offset])])\n",
    "\n",
    "print(f\"{len(text) = }\")\n",
    "print(f\"{len(X)    = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size=len(chars), embedding_dim=5):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(num_embeddings=self.vocab_size,\n",
    "                                      embedding_dim=self.embedding_dim)\n",
    "        self.linear = nn.Linear(in_features=self.embedding_dim,\n",
    "                                out_features=self.vocab_size)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.linear.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 001] 3.18345\r"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "skipgram = SkipGram()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(skipgram.parameters(), lr=0.1)\n",
    "\n",
    "skipgram.train()\n",
    "for e in range(1):\n",
    "    running_loss = 0\n",
    "    count = 0\n",
    "    for x, y in DataLoader(X, batch_size=32, shuffle=True):\n",
    "        count += 1\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_hat = skipgram(x)\n",
    "        loss = criterion(y_hat, y) # <- computing every skip-grams is expensive. Cross entropy loss of huge vocab size (expensive!) * context size\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    print(f\"[epoch {e+1:0>3}] {running_loss/count:7.5f}\", end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skip-gram (with negative sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(text) = 1093\n",
      "len(X)    = 26136\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "text = \"\"\"John quickly realized that the fox was jumping over a brown fence. Meanwhile, the lazy dog slept under the warm sun, dreaming of chasing squirrels in the park. A wizard in a distant land cast spells to levitate objects and summon mystical creatures. The gym was full of athletes lifting weights, running on treadmills, and practicing yoga poses. Buzzing bees were collecting nectar from vibrant flowers, while a group of birds sang harmoniously from the treetops. In the city, cars zoomed by as people hurried to work, their minds filled with tasks and deadlines. The library was a sanctuary of knowledge, where students pored over books and researchers delved into ancient manuscripts. A chef in a bustling kitchen prepared exquisite dishes, skillfully chopping vegetables and grilling meats. At the beach, waves crashed against the shore as children built sandcastles and surfers rode the swells. In the forest, a lumberjack wielded his axe, cutting down trees for timber. The night sky was a tapestry of stars, constellations, and planets, inspiring wonder and awe in all who gazed upon it.\"\"\"\n",
    "text = text.lower()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "c2i = lambda c: chars.index(c)\n",
    "i2c = lambda i: chars[i]\n",
    "\n",
    "X = []\n",
    "context_size = 2\n",
    "for i in range(context_size, len(text)-context_size):\n",
    "    target = c2i(text[i])\n",
    "\n",
    "    positive_samples = []\n",
    "    for offset in range(-context_size, context_size+1):\n",
    "        if offset == 0:\n",
    "            continue\n",
    "        i = c2i(text[i+offset])\n",
    "        positive_samples.append(i)\n",
    "\n",
    "    negative_sample_candidates = list(set(range(len(chars))) - set(positive_samples))\n",
    "    negative_samples = random.sample(negative_sample_candidates, context_size*2*5) # 1:5 ratio of negative sampling\n",
    "\n",
    "    for s in positive_samples:\n",
    "        X.append([target, s, 1])\n",
    "\n",
    "    for s in negative_samples:\n",
    "        X.append([target, s, 0])\n",
    "\n",
    "print(f\"{len(text) = }\")\n",
    "print(f\"{len(X)    = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://wikidocs.net/images/page/69141/그림7.PNG\" height=300/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debiasing word embeddings\n",
    "\n",
    "One way to debias some word embeddings (doctor:man=nurse:woman) is to find a non-biased dimension where is perpendicular to gender axis and project them.\n",
    "\n",
    "<img src=\"src/debiasing.png\" height=400 />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
