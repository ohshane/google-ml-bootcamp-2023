{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word representation\n",
    "The easiest way to embed a word is one-hot.\n",
    "\n",
    "$$\n",
    "\\mathbf x =\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "1 \\\\\n",
    "\\vdots \\\\\n",
    "0 \n",
    "\\end{bmatrix}_{||V|| \\times 1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analogy of two vectors\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "e_{\\text{man}} - e_{\\text{woman}} &\\approx e_{\\text{king}} - e \\\\\n",
    "e &\\approx e_{\\text{king}} - e_{\\text{man}} + e_{\\text{woman}} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The similarity of the vectors are calculated with cosine similarity.\n",
    "\n",
    "$$\n",
    "-1 \\leq\\mathrm{cossim}(e, e_{\\text{king}} - e_{\\text{man}} + e_{\\text{woman}}) \\leq 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# embedding matrix\n",
    "E = np.random.randn(10, 100)\n",
    "\n",
    "# one-hot vector \n",
    "x = np.zeros((100,1))\n",
    "x[42] += 1\n",
    "\n",
    "# embedded vector\n",
    "e = E @ x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(text) = 1093\n",
      "len(X)    = 4356\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"John quickly realized that the fox was jumping over a brown fence. Meanwhile, the lazy dog slept under the warm sun, dreaming of chasing squirrels in the park. A wizard in a distant land cast spells to levitate objects and summon mystical creatures. The gym was full of athletes lifting weights, running on treadmills, and practicing yoga poses. Buzzing bees were collecting nectar from vibrant flowers, while a group of birds sang harmoniously from the treetops. In the city, cars zoomed by as people hurried to work, their minds filled with tasks and deadlines. The library was a sanctuary of knowledge, where students pored over books and researchers delved into ancient manuscripts. A chef in a bustling kitchen prepared exquisite dishes, skillfully chopping vegetables and grilling meats. At the beach, waves crashed against the shore as children built sandcastles and surfers rode the swells. In the forest, a lumberjack wielded his axe, cutting down trees for timber. The night sky was a tapestry of stars, constellations, and planets, inspiring wonder and awe in all who gazed upon it.\"\"\"\n",
    "text = text.lower()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "c2i = lambda c: chars.index(c)\n",
    "i2c = lambda i: chars[i]\n",
    "\n",
    "X = []\n",
    "context_size = 2\n",
    "for i in range(context_size, len(text)-context_size):\n",
    "    for offset in range(-context_size, context_size+1):\n",
    "        if offset == 0:\n",
    "            continue\n",
    "        X.append([c2i(text[i]), c2i(text[i+offset])])\n",
    "\n",
    "print(f\"{len(text) = }\")\n",
    "print(f\"{len(X)    = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/shane/workspace/google-ml-bootcamp-2023/05-RecurrentNeuralNetworks/Week02.ipynb Cell 8\u001b[0m line \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/shane/workspace/google-ml-bootcamp-2023/05-RecurrentNeuralNetworks/Week02.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/shane/workspace/google-ml-bootcamp-2023/05-RecurrentNeuralNetworks/Week02.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/shane/workspace/google-ml-bootcamp-2023/05-RecurrentNeuralNetworks/Week02.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mSkipGram\u001b[39;00m(nn\u001b[39m.\u001b[39mModule):\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2022.05/lib/python3.9/site-packages/torch/__init__.py:229\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[39mif\u001b[39;00m USE_GLOBAL_DEPS:\n\u001b[1;32m    228\u001b[0m         _load_global_deps()\n\u001b[0;32m--> 229\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_C\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m  \u001b[39m# noqa: F403\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[39m# Appease the type checker; ordinarily this binding is inserted by the\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[39m# torch._C module initialization code in C\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[39mif\u001b[39;00m TYPE_CHECKING:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size=len(chars), embedding_dim=5):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(num_embeddings=self.vocab_size,\n",
    "                                      embedding_dim=self.embedding_dim)\n",
    "        self.linear = nn.Linear(in_features=self.embedding_dim,\n",
    "                                out_features=self.vocab_size)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.linear.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "skipgram = SkipGram()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(skipgram.parameters(), lr=0.1)\n",
    "\n",
    "skipgram.train()\n",
    "for e in range(1):\n",
    "    running_loss = 0\n",
    "    count = 0\n",
    "    for x, y in DataLoader(X, batch_size=32, shuffle=True):\n",
    "        count += 1\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_hat = skipgram(x)\n",
    "        loss = criterion(y_hat, y) # <- computing every skip-grams is expensive. Cross entropy loss of huge vocab size (expensive!) * context size\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    print(f\"[epoch {e+1:0>3}] {running_loss/count:7.5f}\", end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skip-gram (with negative sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "text = \"\"\"John quickly realized that the fox was jumping over a brown fence. Meanwhile, the lazy dog slept under the warm sun, dreaming of chasing squirrels in the park. A wizard in a distant land cast spells to levitate objects and summon mystical creatures. The gym was full of athletes lifting weights, running on treadmills, and practicing yoga poses. Buzzing bees were collecting nectar from vibrant flowers, while a group of birds sang harmoniously from the treetops. In the city, cars zoomed by as people hurried to work, their minds filled with tasks and deadlines. The library was a sanctuary of knowledge, where students pored over books and researchers delved into ancient manuscripts. A chef in a bustling kitchen prepared exquisite dishes, skillfully chopping vegetables and grilling meats. At the beach, waves crashed against the shore as children built sandcastles and surfers rode the swells. In the forest, a lumberjack wielded his axe, cutting down trees for timber. The night sky was a tapestry of stars, constellations, and planets, inspiring wonder and awe in all who gazed upon it.\"\"\"\n",
    "text = text.lower()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "c2i = lambda c: chars.index(c)\n",
    "i2c = lambda i: chars[i]\n",
    "\n",
    "X = []\n",
    "context_size = 2\n",
    "for i in range(context_size, len(text)-context_size):\n",
    "    target = c2i(text[i])\n",
    "\n",
    "    positive_samples = []\n",
    "    for offset in range(-context_size, context_size+1):\n",
    "        if offset == 0:\n",
    "            continue\n",
    "        i = c2i(text[i+offset])\n",
    "        positive_samples.append(i)\n",
    "\n",
    "    negative_sample_candidates = list(set(range(len(chars))) - set(positive_samples))\n",
    "    negative_samples = random.sample(negative_sample_candidates, context_size*2*5) # 1:5 ratio of negative sampling\n",
    "\n",
    "    for s in positive_samples:\n",
    "        X.append([target, s, 1])\n",
    "\n",
    "    for s in negative_samples:\n",
    "        X.append([target, s, 0])\n",
    "\n",
    "print(f\"{len(text) = }\")\n",
    "print(f\"{len(X)    = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://wikidocs.net/images/page/69141/그림7.PNG\" height=300/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe (Global vectors for word representation)\n",
    "\n",
    "Co-occurance of words in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a tiny corpus\n",
    "corpus = [\"I like machine learning\",\n",
    "          \"I like coding in Python\",\n",
    "          \"I enjoy learning new things\"]\n",
    "\n",
    "vocab = set()\n",
    "for sentence in corpus:\n",
    "    vocab.update(sentence.lower().split())\n",
    "vocab = sorted(list(vocab))\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "co_occurrence_matrix = np.zeros((vocab_size, vocab_size))\n",
    "\n",
    "for sentence in corpus:\n",
    "    words = sentence.lower().split()\n",
    "    for i, word1 in enumerate(words):\n",
    "        for j, word2 in enumerate(words):\n",
    "            if word1 != word2:\n",
    "                co_occurrence_matrix[vocab.index(word1), vocab.index(word2)] += 1\n",
    "\n",
    "fig, ax = plt.subplots(1,4,figsize=(20,4))\n",
    "ax[0].set_title('co-occurrence matrix')\n",
    "ax[0].imshow(co_occurrence_matrix, cmap='binary_r')\n",
    "ax[0].xaxis.set_ticks_position('top')\n",
    "ax[0].set_xticks(range(len(vocab)))\n",
    "ax[0].set_xticklabels(vocab, rotation=90)\n",
    "ax[0].set_yticks(range(len(vocab)))\n",
    "ax[0].set_yticklabels(vocab)\n",
    "\n",
    "U, S, V_T = np.linalg.svd(co_occurrence_matrix)\n",
    "ax[1].set_title('$U$ (word vector)')\n",
    "ax[1].imshow(U, cmap='binary_r')\n",
    "ax[1].xaxis.set_ticks_position('top')\n",
    "ax[1].set_xticks(range(len(vocab)))\n",
    "ax[1].set_xticklabels(vocab, rotation=90)\n",
    "ax[1].set_yticks([])\n",
    "\n",
    "ax[2].set_title('$S$')\n",
    "ax[2].imshow(np.diag(S), cmap='binary_r')\n",
    "ax[2].set_xticks([])\n",
    "ax[2].set_yticks(range(len(vocab)))\n",
    "ax[2].set_yticklabels(vocab)\n",
    "\n",
    "ax[3].set_title('$V^\\intercal$')\n",
    "ax[3].imshow(V_T, cmap='binary_r')\n",
    "ax[3].set_xticks([])\n",
    "ax[3].set_yticks(range(len(vocab)))\n",
    "ax[3].set_yticklabels(vocab)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debiasing word embeddings\n",
    "\n",
    "One way to debias some word embeddings (doctor:man=nurse:woman) is to find a non-biased dimension where is perpendicular to gender axis and project them.\n",
    "\n",
    "<img src=\"src/debiasing.png\" height=400 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>229985</th>\n",
       "      <td>e.t.c.</td>\n",
       "      <td>0.30635</td>\n",
       "      <td>-1.047300</td>\n",
       "      <td>-0.302170</td>\n",
       "      <td>-0.192570</td>\n",
       "      <td>-1.03850</td>\n",
       "      <td>-0.71002</td>\n",
       "      <td>0.278730</td>\n",
       "      <td>0.338580</td>\n",
       "      <td>-0.147850</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.384140</td>\n",
       "      <td>-1.238200</td>\n",
       "      <td>0.375450</td>\n",
       "      <td>-1.321600</td>\n",
       "      <td>0.158480</td>\n",
       "      <td>0.047650</td>\n",
       "      <td>-0.023359</td>\n",
       "      <td>-0.905180</td>\n",
       "      <td>1.34670</td>\n",
       "      <td>1.27920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289528</th>\n",
       "      <td>2137</td>\n",
       "      <td>-0.27732</td>\n",
       "      <td>-0.156750</td>\n",
       "      <td>-0.028893</td>\n",
       "      <td>-0.114710</td>\n",
       "      <td>-0.57858</td>\n",
       "      <td>-0.68888</td>\n",
       "      <td>0.505910</td>\n",
       "      <td>-0.034682</td>\n",
       "      <td>0.069475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.275080</td>\n",
       "      <td>-0.509240</td>\n",
       "      <td>-0.275620</td>\n",
       "      <td>0.426450</td>\n",
       "      <td>-0.075152</td>\n",
       "      <td>0.505060</td>\n",
       "      <td>0.272850</td>\n",
       "      <td>0.098014</td>\n",
       "      <td>0.27318</td>\n",
       "      <td>0.18083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235861</th>\n",
       "      <td>discoursing</td>\n",
       "      <td>-0.44085</td>\n",
       "      <td>-0.093508</td>\n",
       "      <td>-0.742410</td>\n",
       "      <td>-0.397990</td>\n",
       "      <td>-0.14382</td>\n",
       "      <td>-0.23926</td>\n",
       "      <td>0.092085</td>\n",
       "      <td>-0.087855</td>\n",
       "      <td>-0.098514</td>\n",
       "      <td>...</td>\n",
       "      <td>0.123970</td>\n",
       "      <td>-0.368530</td>\n",
       "      <td>0.024875</td>\n",
       "      <td>0.785340</td>\n",
       "      <td>-0.682100</td>\n",
       "      <td>-0.345760</td>\n",
       "      <td>1.082400</td>\n",
       "      <td>0.386690</td>\n",
       "      <td>0.29955</td>\n",
       "      <td>-1.22470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284668</th>\n",
       "      <td>actualizing</td>\n",
       "      <td>0.10355</td>\n",
       "      <td>0.079821</td>\n",
       "      <td>-0.316390</td>\n",
       "      <td>-0.973870</td>\n",
       "      <td>-0.45961</td>\n",
       "      <td>-0.30537</td>\n",
       "      <td>1.085700</td>\n",
       "      <td>-0.110770</td>\n",
       "      <td>0.599110</td>\n",
       "      <td>...</td>\n",
       "      <td>0.273570</td>\n",
       "      <td>-0.456990</td>\n",
       "      <td>-0.047865</td>\n",
       "      <td>0.421220</td>\n",
       "      <td>0.142690</td>\n",
       "      <td>-0.424830</td>\n",
       "      <td>0.441400</td>\n",
       "      <td>0.327500</td>\n",
       "      <td>0.10860</td>\n",
       "      <td>-0.11897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237748</th>\n",
       "      <td>cubensis</td>\n",
       "      <td>0.29420</td>\n",
       "      <td>-0.569710</td>\n",
       "      <td>-0.479500</td>\n",
       "      <td>0.010294</td>\n",
       "      <td>0.50231</td>\n",
       "      <td>0.19395</td>\n",
       "      <td>0.990630</td>\n",
       "      <td>-0.212240</td>\n",
       "      <td>-0.362770</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.912100</td>\n",
       "      <td>-0.306480</td>\n",
       "      <td>-0.371500</td>\n",
       "      <td>-0.779410</td>\n",
       "      <td>0.202650</td>\n",
       "      <td>0.079314</td>\n",
       "      <td>-0.618540</td>\n",
       "      <td>0.300880</td>\n",
       "      <td>0.30027</td>\n",
       "      <td>1.60140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303066</th>\n",
       "      <td>kezi</td>\n",
       "      <td>-0.12759</td>\n",
       "      <td>-0.227090</td>\n",
       "      <td>0.234360</td>\n",
       "      <td>0.499230</td>\n",
       "      <td>-0.73960</td>\n",
       "      <td>0.31408</td>\n",
       "      <td>-0.008921</td>\n",
       "      <td>0.595970</td>\n",
       "      <td>0.127430</td>\n",
       "      <td>...</td>\n",
       "      <td>0.210970</td>\n",
       "      <td>-0.051466</td>\n",
       "      <td>-0.225720</td>\n",
       "      <td>-0.334390</td>\n",
       "      <td>-0.249560</td>\n",
       "      <td>0.265710</td>\n",
       "      <td>-0.321470</td>\n",
       "      <td>-0.295670</td>\n",
       "      <td>0.44369</td>\n",
       "      <td>0.38995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78013</th>\n",
       "      <td>fraph</td>\n",
       "      <td>1.03880</td>\n",
       "      <td>-0.240660</td>\n",
       "      <td>0.556360</td>\n",
       "      <td>-0.240250</td>\n",
       "      <td>0.71568</td>\n",
       "      <td>-0.97994</td>\n",
       "      <td>0.642600</td>\n",
       "      <td>-0.650500</td>\n",
       "      <td>-0.089445</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.153530</td>\n",
       "      <td>-0.585180</td>\n",
       "      <td>-0.516300</td>\n",
       "      <td>-0.782170</td>\n",
       "      <td>0.308690</td>\n",
       "      <td>-0.217080</td>\n",
       "      <td>-0.127580</td>\n",
       "      <td>0.133970</td>\n",
       "      <td>-0.25089</td>\n",
       "      <td>-0.71658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16695</th>\n",
       "      <td>mcdowell</td>\n",
       "      <td>-0.37532</td>\n",
       "      <td>-0.343330</td>\n",
       "      <td>-0.335120</td>\n",
       "      <td>0.984910</td>\n",
       "      <td>0.25654</td>\n",
       "      <td>0.25304</td>\n",
       "      <td>-0.546930</td>\n",
       "      <td>0.320400</td>\n",
       "      <td>-0.128310</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.060907</td>\n",
       "      <td>0.688770</td>\n",
       "      <td>-0.508530</td>\n",
       "      <td>0.129620</td>\n",
       "      <td>-0.153290</td>\n",
       "      <td>0.295970</td>\n",
       "      <td>0.171770</td>\n",
       "      <td>-0.230180</td>\n",
       "      <td>-0.40340</td>\n",
       "      <td>0.45920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326348</th>\n",
       "      <td>nedelciu</td>\n",
       "      <td>-0.41663</td>\n",
       "      <td>-0.552270</td>\n",
       "      <td>-0.208120</td>\n",
       "      <td>0.155780</td>\n",
       "      <td>-0.50325</td>\n",
       "      <td>0.75483</td>\n",
       "      <td>-0.069079</td>\n",
       "      <td>0.058150</td>\n",
       "      <td>-0.106650</td>\n",
       "      <td>...</td>\n",
       "      <td>0.387740</td>\n",
       "      <td>-0.315890</td>\n",
       "      <td>-1.055000</td>\n",
       "      <td>0.063344</td>\n",
       "      <td>-0.167200</td>\n",
       "      <td>-0.462350</td>\n",
       "      <td>0.206650</td>\n",
       "      <td>0.274440</td>\n",
       "      <td>0.33568</td>\n",
       "      <td>-0.44969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116742</th>\n",
       "      <td>sli</td>\n",
       "      <td>0.27939</td>\n",
       "      <td>-0.706680</td>\n",
       "      <td>0.555550</td>\n",
       "      <td>1.102200</td>\n",
       "      <td>-0.46336</td>\n",
       "      <td>0.33452</td>\n",
       "      <td>1.124100</td>\n",
       "      <td>-0.632990</td>\n",
       "      <td>-0.440500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.201820</td>\n",
       "      <td>-0.392730</td>\n",
       "      <td>-0.473270</td>\n",
       "      <td>0.139770</td>\n",
       "      <td>-0.389120</td>\n",
       "      <td>-0.851620</td>\n",
       "      <td>0.772090</td>\n",
       "      <td>0.200510</td>\n",
       "      <td>0.53654</td>\n",
       "      <td>0.56097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0        1         2         3         4        5        6   \\\n",
       "229985       e.t.c.  0.30635 -1.047300 -0.302170 -0.192570 -1.03850 -0.71002   \n",
       "289528         2137 -0.27732 -0.156750 -0.028893 -0.114710 -0.57858 -0.68888   \n",
       "235861  discoursing -0.44085 -0.093508 -0.742410 -0.397990 -0.14382 -0.23926   \n",
       "284668  actualizing  0.10355  0.079821 -0.316390 -0.973870 -0.45961 -0.30537   \n",
       "237748     cubensis  0.29420 -0.569710 -0.479500  0.010294  0.50231  0.19395   \n",
       "303066         kezi -0.12759 -0.227090  0.234360  0.499230 -0.73960  0.31408   \n",
       "78013         fraph  1.03880 -0.240660  0.556360 -0.240250  0.71568 -0.97994   \n",
       "16695      mcdowell -0.37532 -0.343330 -0.335120  0.984910  0.25654  0.25304   \n",
       "326348     nedelciu -0.41663 -0.552270 -0.208120  0.155780 -0.50325  0.75483   \n",
       "116742          sli  0.27939 -0.706680  0.555550  1.102200 -0.46336  0.33452   \n",
       "\n",
       "              7         8         9   ...        41        42        43  \\\n",
       "229985  0.278730  0.338580 -0.147850  ... -0.384140 -1.238200  0.375450   \n",
       "289528  0.505910 -0.034682  0.069475  ...  0.275080 -0.509240 -0.275620   \n",
       "235861  0.092085 -0.087855 -0.098514  ...  0.123970 -0.368530  0.024875   \n",
       "284668  1.085700 -0.110770  0.599110  ...  0.273570 -0.456990 -0.047865   \n",
       "237748  0.990630 -0.212240 -0.362770  ... -0.912100 -0.306480 -0.371500   \n",
       "303066 -0.008921  0.595970  0.127430  ...  0.210970 -0.051466 -0.225720   \n",
       "78013   0.642600 -0.650500 -0.089445  ... -0.153530 -0.585180 -0.516300   \n",
       "16695  -0.546930  0.320400 -0.128310  ... -0.060907  0.688770 -0.508530   \n",
       "326348 -0.069079  0.058150 -0.106650  ...  0.387740 -0.315890 -1.055000   \n",
       "116742  1.124100 -0.632990 -0.440500  ...  0.201820 -0.392730 -0.473270   \n",
       "\n",
       "              44        45        46        47        48       49       50  \n",
       "229985 -1.321600  0.158480  0.047650 -0.023359 -0.905180  1.34670  1.27920  \n",
       "289528  0.426450 -0.075152  0.505060  0.272850  0.098014  0.27318  0.18083  \n",
       "235861  0.785340 -0.682100 -0.345760  1.082400  0.386690  0.29955 -1.22470  \n",
       "284668  0.421220  0.142690 -0.424830  0.441400  0.327500  0.10860 -0.11897  \n",
       "237748 -0.779410  0.202650  0.079314 -0.618540  0.300880  0.30027  1.60140  \n",
       "303066 -0.334390 -0.249560  0.265710 -0.321470 -0.295670  0.44369  0.38995  \n",
       "78013  -0.782170  0.308690 -0.217080 -0.127580  0.133970 -0.25089 -0.71658  \n",
       "16695   0.129620 -0.153290  0.295970  0.171770 -0.230180 -0.40340  0.45920  \n",
       "326348  0.063344 -0.167200 -0.462350  0.206650  0.274440  0.33568 -0.44969  \n",
       "116742  0.139770 -0.389120 -0.851620  0.772090  0.200510  0.53654  0.56097  \n",
       "\n",
       "[10 rows x 51 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "with open('src/glove.6B.50d.txt', 'r') as f:\n",
    "    rows = f.readlines()\n",
    "\n",
    "rows = [row.split() for row in rows]\n",
    "df = pd.DataFrame(rows)\n",
    "df = df.apply(lambda col: pd.to_numeric(col, errors='ignore'), axis=0)\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import *\n",
    "\n",
    "def _vectorize(word):\n",
    "    word = word.lower()\n",
    "    if word in df[0].tolist():\n",
    "        vector = df[df[0] == word].iloc[:,1:]\n",
    "        vector = vector.to_numpy().flatten()\n",
    "    else:\n",
    "        vector = np.zeros(df.shape[1]-1)\n",
    "    return vector\n",
    "\n",
    "def vectorize(words: Union[List[str], str]):\n",
    "    if type(words) is list:\n",
    "        words = list(map(_vectorize, words))\n",
    "        words = np.stack(words, axis=0)\n",
    "        return words\n",
    "    return _vectorize(words)\n",
    "\n",
    "def cossim(u, v):\n",
    "    return u @ v / (np.linalg.norm(u) * np.linalg.norm(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4837982694403796, 0.29868826318084823)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cossim(*vectorize('man game'.split())), cossim(*vectorize('woman game'.split())) # -> gender bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game            -0.33507 ♂\n",
      "lipstick         0.27692 ♀\n",
      "guns            -0.18885 ♂\n",
      "science         -0.06083 ♂\n",
      "arts             0.00819 ♀\n",
      "literature       0.06473 ♀\n",
      "warrior         -0.20920 ♂\n",
      "doctor           0.11895 ♀\n",
      "tree            -0.07089 ♂\n",
      "receptionist     0.33078 ♀\n",
      "technology      -0.13194 ♂\n",
      "fashion          0.03564 ♀\n",
      "teacher          0.17921 ♀\n",
      "engineer        -0.08039 ♂\n",
      "pilot            0.00108 ♀\n",
      "computer        -0.10330 ♂\n",
      "singer           0.18501 ♀\n"
     ]
    }
   ],
   "source": [
    "g = vectorize('woman') - vectorize('man') # man -> woman\n",
    "\n",
    "words = ['game', 'lipstick', 'guns', 'science', 'arts', 'literature',\n",
    "         'warrior','doctor', 'tree', 'receptionist', 'technology', \n",
    "         'fashion', 'teacher', 'engineer', 'pilot', 'computer', 'singer']\n",
    "\n",
    "for word in words:\n",
    "    e = vectorize(word)\n",
    "    sim = cossim(e,g)\n",
    "    print(f\"{word:15} {sim:>8.5f} {'♂' if sim < 0 else '♀'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debiasing technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neutralize\n",
    "<img src=\"src/neutralize.png\" width=800 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.3350731148943847, 1.15603655367518e-16)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def neutralize(e, g): # debiasing non-gender-related words\n",
    "    g_unit = g / np.linalg.norm(g)\n",
    "    e_biased_factor = np.dot(e, g_unit) * g_unit\n",
    "    e_debiased = e - e_biased_factor\n",
    "    return e_debiased\n",
    "\n",
    "g = vectorize('woman') - vectorize('man')\n",
    "e = vectorize('game')\n",
    "\n",
    "e_neutralized = neutralize(e, g)\n",
    "cossim(e, g), cossim(e_neutralized, g) # near 0!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Equalize\n",
    "<img src=\"src/equalize.png\" width=800 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7787298355385984, 0.9999999999999999)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def equalize(e1, e2, g): # equalizing genderness of grammatical gender words\n",
    "    e_mu = (e1 + e2) / 2\n",
    "    g_unit = g / np.linalg.norm(g)\n",
    "    e_mu_biased_factor = np.dot(e_mu, g_unit) * g_unit\n",
    "    e_mu_orthogonal = e_mu - e_mu_biased_factor # lying on the g_orthogonal axis\n",
    "\n",
    "    e1_biased_factor = np.dot(e1, g_unit) * g_unit # lying on the g axis\n",
    "    e2_biased_factor = np.dot(e2, g_unit) * g_unit # lying on the g axis\n",
    "\n",
    "    e1_corrected = e1_biased_factor + e_mu_orthogonal\n",
    "    e2_corrected = e2_biased_factor + e_mu_orthogonal\n",
    "\n",
    "    return e1_corrected, e2_corrected\n",
    "\n",
    "g = vectorize('woman') - vectorize('man')\n",
    "e1, e2 = vectorize('actor'), vectorize('actress')\n",
    "\n",
    "e1_corrected, e2_corrected = equalize(e1, e2, g)\n",
    "\n",
    "cossim(e2-e1, g), cossim(e2_corrected-e1_corrected, g) # near 1!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
